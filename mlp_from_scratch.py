# -*- coding: utf-8 -*-
"""MLP from scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1URzxk-F5rgdwqr2OXGxKLj4LZwElMz3p
"""

#Defining a multi layer Perceptron with one hidden layer and one output layer we define it as a class
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

class MLP:
  def __init__(self,input_dim,hidden_dim,output_dim,hidden_activation = "relu", output_activation="softmax"):
    """
    Initialize an MLP with the given input, output, hidden layer and activation functions
      parameters:
      input_dim (int): the number of the futures
      hidden_dim(int) : the number of hidden units
      output_dim (int): the number of output classes
      hidden_activation (str) : the activation function used in the hidden layer
      output_activation (str): the activation function used in the output layer
    """
    self.input_dim = input_dim
    self.hidden_dim = hidden_dim
    self.output_dim = output_dim

    self.hidden_acivation = hidden_activation
    self.output_activation = output_activation

    self.w1 = np.random.randn(input_dim,hidden_dim)
    self.b1 = np.random.randn(hidden_dim)

    self.w2 = np.random.randn(hidden_dim,output_dim)
    self.b2 = np.random.randn(output_dim)

    self.a1 = None #this one is going to be used for the activation function

  def activation_function(self,x,activation):
    if activation == "sigmoid":
      return 1/(1 + np.exp(-x))  #mathematical definiton of sigmoid function

    elif activation == "relu":
      return np.maximum(0,x)

    elif activation == "tanh":
      return np.tanh(x)

    elif activation == "leakyrelu":
      return np.maximum(0.1 * x, x)

    elif activation == "softmax":
      exp_x = np.exp(x - np.max(x, axis = 1, keepdims=True))
      return exp_x / (np.sum(exp_x,axis=1,keepdims=True))

    else:
      raise ValueError(f"Unknown activation function: {activation}")

  #defining the forward functiion
  def forward(self,x):
    """
     performs the forward propagation through the MLP
     X (ndarray) : a matrix of shape (num_sample, input_dim)

     returns:
       ndaaray: a matrix of shape (num_sample, out_dim) contains prediction probabilities of each class
    """

    z1 = np.dot(x, self.w1) + self.b1 # ouput of the first layer
    self.a1 = self.activation_function(z1, self.hidden_acivation)

    z2 = np.dot(self.a1, self.w2) + self.b2
    y_hat = self.activation_function(z2, self.output_activation)

    return y_hat


  def backward(self, x, y, y_hat, lr):
    #compute the gradient of the loss function with respect to  the cross-enropy loss with respect to the output layer
    delta2 = y_hat - y

    #compute the gradient of the cross-entopy loss with respect to the hidden layer activation
    delta1 = np.dot(delta2, np.transpose(self.w2))
    delta1[self.a1 <=0] = 0 #gradient of Relu Function

    #compute the gradient of the cross-entropy loss with respect to the weights and biases
    grad_w2 = np.dot(np.transpose(self.a1), delta2)
    grad_b2 = np.sum(delta2, axis=0)


    grad_w1 = np.dot(np.transpose(x),delta1)
    grad_b1 = np.sum(delta1, axis = 0)

    #clip the gradient to prevent the exploding gradient or vanishing gradient
    max_grad = 5.0
    grad_w2 = np.clip(grad_w2, -max_grad, max_grad)
    grad_b2 = np.clip(grad_b2, -max_grad, max_grad)
    grad_w1 = np.clip(grad_w1, -max_grad, max_grad)
    grad_b1 = np.clip(grad_b1, -max_grad, max_grad)


    #update the weights and biases using gradient decent
    self.w2 -= lr * grad_w2
    self.b2 -= lr * grad_b2
    self.w1 -= lr * grad_w1
    self.b1 -= lr * grad_b1


  #define fit function that is going to use the forward and backward

  def fit(self,X_train, y_train_onehot, epochs, lr):
    """
    Trains the MLP using the backpropagation

    y_train_oehot : one-hot encoded of the target labels
    """
    losses = []
    for i in range(epochs):
      #forward pass
      y_hat = self.forward(X_train)

      #compute the cross-entropy loss and add it to losses list
      loss = -np.sum(y_train_onehot * np.log(y_hat)) / (X_train.shape[0])
      losses.append(loss)

      #backward pass
      self.backward(X_train,y_train_onehot, y_hat, lr)

      #print the losee in every epochs
      print(f"Epoch {i}: Loss = {loss}")

    #plot the learning curve
    plt.plot(losses)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Learning curve")
    plt.show()

  def predict(self, x):
    y_hat = self.forward(x)
    return np.argmax(y_hat, axis = 1)

salary = pd.read_csv("https://raw.githubusercontent.com/Moulishankar10/Employee-Salary-Classification/main/data/salary.csv");
salary.head(5)

import seaborn as sns
sns.pairplot(salary, hue="salary_range")

#lets do splitting of the data

from sklearn.model_selection import train_test_split

X = salary.iloc[:,:-1]
y = salary.iloc[:,-1]

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=42)

#normalize the data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

#Encodding the target labels
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()

y_train = encoder.fit_transform(y_train)
y_test =encoder.transform(y_test)
y_train

#conver the target variable to one-hot encoding
from keras.utils import to_categorical

y_train_onehot = to_categorical(y_train, num_classes=3)
y_train_onehot

input_dim = X_train.shape[1]
hidden_dim = 10
output_dim = 3
y_train_onehot = to_categorical(y_train, num_classes=3)

mlp = MLP(input_dim, hidden_dim, output_dim, hidden_activation="relu")

mlp.fit(X_train,y_train_onehot, epochs=100, lr=0.01)

from sklearn.metrics import accuracy_score
y_pred = mlp.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(accuracy)